{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will be concerned with demonstrating the following course contents:\n",
    "* Relational databases (SQL) and MongoDB (NoSQL)\n",
    "* Graph Databases\n",
    "* Streaming algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL databases has over the last decades been the core norm of persisting data into non-volatile memory. Although SQL is a great way to represent certain data sets, it can be quite time expensive when queries starts to compute a high number of large cartesian products between tables. To mitigate some of these performance problems, there has been an increasing interest within the area of NoSQL (i.e. a database that is not meant to have normalized relations). MongoDB is a popular NoSQL database and provides a schema-less persistency of the data in a json-like format (called bson). In contrary to SQL, the data in a NoSQL database should be denormalized as much as possible (that is, you should store it wihin a nested structure). This makes MongoDB especially interesting when storing large documents or highly detailed log-files with a high throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we're going to use the Northwind database. To give the reader a idea of how this database looks like, from a relational perspective, a ER diagram is displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i3.codeplex.com/download?ProjectName=northwinddatabase&DownloadId=269240\" width=\"800\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent MongoDB database, that we are going to use, has onfortunately not been denormalized into one collection. Because of this, the way MongoDB is being used in this exercise is somehow _a special case_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task: Establish connection to MongoDB and SQLite_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first exercise, we are going to make a basic connection to both SQLite (a light-weight SQL engine that persists data into files) and to MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do, is to import sqlite3, which is the Python module we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a small function that returns a connection to the northwind.db (a file in the same directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sqlite_connection():\n",
    "    conn = lite.connect('northwind.db')\n",
    "    conn.text_factory = str\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, getting a connection to the SQLite DB is easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = get_sqlite_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the `conn` object we can create a cursor, which is used to query the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cursor `cur` we can can retrieve all the products by making the sql query `SELECT * FROM PRODUCTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1046a12d0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM PRODUCTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query has been executed and the result set can be obtained by calling fetchall. Here, we are just going to see the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Chai', 1, 1, '10 boxes x 20 bags', 18, 39, 0, 10, '0')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.fetchall()[:1] # Take the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the database once again, this time, however, we are going to sort the result set using the `customerid`, which is a column in the table `CUSTOMERS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ALFKI',\n",
       "  'Alfreds Futterkiste',\n",
       "  'Maria Anders',\n",
       "  'Sales Representative',\n",
       "  'Obere Str. 57',\n",
       "  'Berlin',\n",
       "  None,\n",
       "  '12209',\n",
       "  'Germany',\n",
       "  '030-0074321',\n",
       "  '030-0076545')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM customers ORDER BY customerid ASC')\n",
    "cur.fetchall()[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to MongoDB is slightly more difficult, than connecting to a SQLite DB, since MongoDB is a client-server database engine and has to be configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install MongoDB. After MongoDB is installed, we need to run a shell script that imports the northwind database into MongoDB. After this, we can start the MongoDB service on localhost and the default port using `mongod` in bash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mongod-service-start.png\" width=\"800\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've done this, we can `pip install pymongo`, and we are finally ready to start interacting with Northwind MongoDB through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function, that connects to MongoDB and then returns a connection to \"Nortwind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mongo_connection():\n",
    "    client=MongoClient('localhost',27017)\n",
    "    return client['Northwind']   # Get the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can easily make return an MongoDB conn instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = get_mongo_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established a connection and can use it to retrieve all the products and list the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'CategoryID': 1,\n",
       " u'Discontinued': 0,\n",
       " u'ProductID': 1,\n",
       " u'ProductName': u'Chai',\n",
       " u'QuantityPerUnit': u'10 boxes x 20 bags',\n",
       " u'ReorderLevel': 10,\n",
       " u'SupplierID': 1,\n",
       " u'UnitPrice': 18.0,\n",
       " u'UnitsInStock': 39,\n",
       " u'UnitsOnOrder': 0,\n",
       " u'_id': ObjectId('5609b12899ded9537b06c7e5')}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = c.products.find()\n",
    "list(products)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, this is just a dict, it can be easily seen as a bson object, which is the original way that MongoDB persists data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to make yet another query, but this time we wanted to sort the result, we can apply a sort filter to a specific bson key. From here on, we're going to use Pandas DataFrame since they provide an easy way of pretty-printing tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALFKI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANATR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AROUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BLAUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BLONP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOLID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BONAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BOTTM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CustomerID\n",
       "0      ALFKI\n",
       "1      ANATR\n",
       "2      ANTON\n",
       "3      AROUT\n",
       "4      BERGS\n",
       "5      BLAUS\n",
       "6      BLONP\n",
       "7      BOLID\n",
       "8      BONAP\n",
       "9      BOTTM"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_mongo_connection()\n",
    "customers = c.customers.find().sort('CustomerID', pymongo.ASCENDING)\n",
    "top10 = [customer['CustomerID'] for customer in customers][:10]\n",
    "pd.DataFrame(top10, columns=['CustomerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above shows the top 10 customer ids, sorted in ascending order, from the customer collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM customers AS c INNER JOIN orders AS o ON c.customerid=o.customerid where o.customerid='ALFKI'\")\n",
    "pd.DataFrame(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for customer in c.customers.find({'CustomerID':'ALFKI'}):\n",
    "    for order in c.orders.find({'CustomerID': customer['CustomerID']}):\n",
    "        for details in c['order-details'].find({'OrderID': order['OrderID']}):\n",
    "            joined_result = order.copy()\n",
    "            joined_result.update(details)\n",
    "r = cur.fetchall()\n",
    "\n",
    "# Pretty print using pandas\n",
    "pd.DataFrame([joined_result.values()], columns=joined_result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "_Get all orders (with products) made by ALFKI that contain at least 2 products._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM customers AS c INNER JOIN orders AS o ON c.customerid=o.customerid '\n",
    "                    ' INNER JOIN \"Order Details\" AS od ON od.orderid=o.orderid '\n",
    "                    ' INNER JOIN products AS p ON p.productid=od.productid '\n",
    "                    \" WHERE c.customerid='ALFKI'\"\n",
    "                    ' GROUP BY o.orderid HAVING count(distinct od.productid) >= 2')\n",
    "r = cur.fetchall()\n",
    "pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson.code import Code\n",
    "c = get_mongo_connection()\n",
    "reducer = Code(\n",
    "        \"\"\"\n",
    "            function(obj, prev){\n",
    "              prev.count++;\n",
    "            }\n",
    "        \"\"\")\n",
    "orders = []\n",
    "for customer in c.customers.find({'CustomerID':'ALFKI'}):\n",
    "    for order in c.orders.find({'CustomerID': customer['CustomerID']}):\n",
    "        results = c['order-details'].group(key={\"OrderID\": 1}, condition={}, initial={\"count\": 0}, reduce=reducer)\n",
    "        for details in results:\n",
    "            if details['OrderID'] == order['OrderID'] and details['count'] >= 2:\n",
    "                orders.append(order)\n",
    "                joined_result = order.copy()\n",
    "                joined_result.update(details)\n",
    "                \n",
    "# Pretty print using pandas\n",
    "pd.DataFrame(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestExercise54(unittest.TestCase):\n",
    "    def test_sqlite_count_products(self):\n",
    "        conn = connect_sqlite()\n",
    "        cur = conn.cursor()\n",
    "        cur.execute('SELECT count(productid) FROM \"Order Details\" where productid=7')\n",
    "        print 'SQLite Count: ' + str(cur.fetchone()[0])\n",
    "\n",
    "    def test_mongo_count_products(self):\n",
    "        c = get_mongo_connection()\n",
    "        print 'MongoDB Count: ' + str(c['order-details'].find({'ProductID': 7}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>ProductName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Chai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>Chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Aniseed Syrup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Chef Anton's Cajun Seasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Chef Anton's Gumbo Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Grandma's Boysenberry Spread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>Uncle Bob's Organic Dried Pears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Northwoods Cranberry Sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Mishi Kobe Niku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>Ikura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>Queso Cabrales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>Queso Manchego La Pastora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>Konbu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>Tofu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>Genen Shouyu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>Pavlova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Alice Mutton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>Carnarvon Tigers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>Teatime Chocolate Biscuits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>Sir Rodney's Marmalade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>Sir Rodney's Scones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>Gustaf's Kn�ckebr�d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>Tunnbr�d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>Guaran� Fant�stica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>NuNuCa Nu�-Nougat-Creme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>Gumb�r Gummib�rchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>Schoggi Schokolade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>R�ssle Sauerkraut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>Th�ringer Rostbratwurst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>Nord-Ost Matjeshering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>Zaanse koeken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>Chocolade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>7</td>\n",
       "      <td>Maxilaku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>Valkoinen suklaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>16</td>\n",
       "      <td>Manjimup Dried Apples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7</td>\n",
       "      <td>Filo Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8</td>\n",
       "      <td>Perth Pasties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>13</td>\n",
       "      <td>Tourti�re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>12</td>\n",
       "      <td>P�t� chinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>18</td>\n",
       "      <td>Gnocchi di nonna Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8</td>\n",
       "      <td>Ravioli Angelo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>Escargots de Bourgogne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>19</td>\n",
       "      <td>Raclette Courdavault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>19</td>\n",
       "      <td>Camembert Pierrot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>Sirop d'�rable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>22</td>\n",
       "      <td>Tarte au sucre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>Vegie-spread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>13</td>\n",
       "      <td>Wimmers gute Semmelkn�del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>12</td>\n",
       "      <td>Louisiana Fiery Hot Pepper Sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>Louisiana Hot Spiced Okra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>14</td>\n",
       "      <td>Scottish Longbreads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9</td>\n",
       "      <td>Gudbrandsdalsost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>13</td>\n",
       "      <td>Outback Lager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>16</td>\n",
       "      <td>Flotemysost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>16</td>\n",
       "      <td>Mozzarella di Giovanni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>R�d Kaviar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5</td>\n",
       "      <td>Longlife Tofu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>17</td>\n",
       "      <td>Rh�nbr�u Klosterbier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>12</td>\n",
       "      <td>Lakkalik��ri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>15</td>\n",
       "      <td>Original Frankfurter gr�ne So�e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    count                       ProductName\n",
       "0       8                              Chai\n",
       "1      18                             Chang\n",
       "2       7                     Aniseed Syrup\n",
       "3       6      Chef Anton's Cajun Seasoning\n",
       "4       6            Chef Anton's Gumbo Mix\n",
       "5       5      Grandma's Boysenberry Spread\n",
       "6      29   Uncle Bob's Organic Dried Pears\n",
       "7       5        Northwoods Cranberry Sauce\n",
       "8       1                   Mishi Kobe Niku\n",
       "9      16                             Ikura\n",
       "10     12                    Queso Cabrales\n",
       "11      7         Queso Manchego La Pastora\n",
       "12     15                             Konbu\n",
       "13      7                              Tofu\n",
       "14      2                      Genen Shouyu\n",
       "15     14                           Pavlova\n",
       "16     16                      Alice Mutton\n",
       "17      9                  Carnarvon Tigers\n",
       "18      9        Teatime Chocolate Biscuits\n",
       "19      7            Sir Rodney's Marmalade\n",
       "20      9               Sir Rodney's Scones\n",
       "21      6               Gustaf's Kn�ckebr�d\n",
       "22      8                          Tunnbr�d\n",
       "23     15                Guaran� Fant�stica\n",
       "24      5           NuNuCa Nu�-Nougat-Creme\n",
       "25     10               Gumb�r Gummib�rchen\n",
       "26      4                Schoggi Schokolade\n",
       "27      8                 R�ssle Sauerkraut\n",
       "28     10           Th�ringer Rostbratwurst\n",
       "29     10             Nord-Ost Matjeshering\n",
       "..    ...                               ...\n",
       "46     10                     Zaanse koeken\n",
       "47      2                         Chocolade\n",
       "48      7                          Maxilaku\n",
       "49      2                  Valkoinen suklaa\n",
       "50     16             Manjimup Dried Apples\n",
       "51      7                          Filo Mix\n",
       "52      8                     Perth Pasties\n",
       "53     13                         Tourti�re\n",
       "54     12                      P�t� chinois\n",
       "55     18            Gnocchi di nonna Alice\n",
       "56      8                    Ravioli Angelo\n",
       "57      4            Escargots de Bourgogne\n",
       "58     19              Raclette Courdavault\n",
       "59     19                 Camembert Pierrot\n",
       "60      7                    Sirop d'�rable\n",
       "61     22                    Tarte au sucre\n",
       "62      7                      Vegie-spread\n",
       "63     13         Wimmers gute Semmelkn�del\n",
       "64     12  Louisiana Fiery Hot Pepper Sauce\n",
       "65      4         Louisiana Hot Spiced Okra\n",
       "66     14               Scottish Longbreads\n",
       "67      9                  Gudbrandsdalsost\n",
       "68     13                     Outback Lager\n",
       "69     16                       Flotemysost\n",
       "70     16            Mozzarella di Giovanni\n",
       "71      4                        R�d Kaviar\n",
       "72      5                     Longlife Tofu\n",
       "73     17              Rh�nbr�u Klosterbier\n",
       "74     12                      Lakkalik��ri\n",
       "75     15   Original Frankfurter gr�ne So�e\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmt = \"\"\"\n",
    "SELECT count(p.productid), p.productname FROM orders\n",
    "INNER JOIN [Order Details] AS od ON od.orderid=orders.orderid\n",
    "INNER JOIN products AS p ON p.productid=od.productid\n",
    "WHERE CustomerID IN\n",
    "            (SELECT CustomerID FROM orders\n",
    "             INNER JOIN [Order Details] ON [Order Details].orderid=orders.orderid\n",
    "             WHERE [Order Details].productid=7)\n",
    "GROUP BY od.productid\n",
    "\"\"\"\n",
    "cur.execute(stmt)\n",
    "#print list(cur.fetchall())\n",
    "df_sql = pd.DataFrame(cur.fetchall(), columns=['count','ProductName'])\n",
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = get_mongo_connection()\n",
    "from bson.code import Code\n",
    "reducer = Code(\n",
    "        \"\"\"\n",
    "            function(obj, prev){\n",
    "              prev.count++;\n",
    "            }\n",
    "        \"\"\")\n",
    "#results = c['order-details'].group(key={\"ProductID\": 1, 'CustomerID':2}, condition={}, initial={\"count\": 0}, reduce=reducer)\n",
    "#for x in results:\n",
    "#    print x\n",
    "customers = {}\n",
    "products = {}\n",
    "for details in c['order-details'].find({'ProductID': 7}):\n",
    "    for order in c.orders.find({'OrderID':details['OrderID']}):\n",
    "        customers[order['CustomerID']]= 0\n",
    "\n",
    "for customer_id in customers.keys():\n",
    "    for r in c.orders.find():\n",
    "        if r['CustomerID'] == customer_id:\n",
    "            for details in c['order-details'].find({'OrderID':r['OrderID']}):\n",
    "\n",
    "                product_name = c.products.find({'ProductID': details['ProductID']})[0]['ProductName']\n",
    "\n",
    "                if product_name in products.keys():\n",
    "                    products[product_name] += 1\n",
    "                else:\n",
    "                    products[product_name] = 1\n",
    "\n",
    "df_mongo = pd.DataFrame(products.keys(), columns=['ProductName']).sort('ProductName')\n",
    "df_mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Streaming Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use streaming algorithms when the given data is of such a size that it can not be stored at all, and where we only get to observe each sample once. Also, Wikipedia adds that *These algorithms have limited memory available to them (much less than the input size) and also limited processing time per item. These constraints may mean that an algorithm produces an approximate answer based on a summary or \"sketch\" of the data stream in memory.*\n",
    "\n",
    "Three of these kind of algorithms will be examined here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1 - Bloom filter\n",
    "\n",
    "The Bloom filter is a probabilistic data structure, that is used to test whether an element is a member of a set. Only false positive matches can occur (the element is shown as a member though it is not - \"possibly in set\", or \"definitely not in the set\"), therefore it is said that a Bloom filter has a 100% recall rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, a such filter is to be built, which is able to check the words from a text written by Shakespeare against a dictionary representing all words of the English language (*shakespeare.txt*, *dict*).\n",
    "\n",
    "#### Implement a Bloom Filter as a Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class bloom_filter:\n",
    "    \n",
    "    # ctor of the filter\n",
    "    def __init__(self, bm, hashf, hashfk):\n",
    "        # initializing the bit array to zeroes\n",
    "        self.__bitarray = numpy.zeros(bm)\n",
    "        self.m = bm\n",
    "        \n",
    "        # taking the hash function. it should take two parameters:\n",
    "        # the string to be hashed, and the parameter of the hash, which will be\n",
    "        # just the no. of it now\n",
    "        self.__hash = hashf\n",
    "        \n",
    "        # storing the number of hash functions\n",
    "        self.__k = hashfk\n",
    "        \n",
    "    def __get_bitz(self, item):\n",
    "        # calls the hash function the given \"k\" times, returns the hash values in a list\n",
    "        # it is divided by % in order to limit the results to the 'm' range\n",
    "        return [self.__hash(item, kc) % self.m for kc in range(self.__k)]\n",
    "    \n",
    "    def add(self, item):\n",
    "        try:\n",
    "            numpy.put(self.__bitarray, self.__get_bitz(item), 1)\n",
    "        except IndexError:\n",
    "            print 'That offset isnt here!'\n",
    "        \n",
    "    def lookup(self, item):\n",
    "        # get the set bit (indexes) for the given item\n",
    "        bitz = self.__get_bitz(item)\n",
    "        \n",
    "        # check it against the stored bitstring\n",
    "        # returns False if not all of the items are nonzero\n",
    "        return numpy.count_nonzero(numpy.take(self.__bitarray, bitz)) == len(bitz)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bloom filter class shown above is quite general - due to the fact, that many of the parameters are up to the user.\n",
    "\n",
    "The desired size of the filter (= hash table size) (*m*), the hash function and the number of hash functions (*k*) are specified when instantiating the class. According to the exercise quite, this time we will use a bitstring with 1000000 (one-million) bits, and the number of the hash functions are calculated as: $m/n * ln(2)$. The *n* in the equation is the expected number of distinct elements in the input (= set size), equals with the length of our English dictionary in lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The exact values then: m = 1000000, n = 235886, k = 3\n"
     ]
    }
   ],
   "source": [
    "import math # due to log() and ceil()\n",
    "\n",
    "m = 1000000\n",
    "n = sum(1 for line in open('dict'))\n",
    "\n",
    "k = int(math.ceil(float(m) / n  * math.log(2)))\n",
    "\n",
    "print 'The exact values then: m = %d, n = %d, k = %d' % (m, n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hash function is the **MurmurHash3** this time, which is just passed to the class as a lambda function. Its second parameter (the index of the given hash function in the Bloom filter) will be used as a seed to the hash function, yielding a different (albeit of course reproduceable) hash for the same input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mmh3\n",
    "\n",
    "# using the MurmurHash3, and the number of the hash function as the seed\n",
    "hash_function = lambda x, y : mmh3.hash(x) if y == None else mmh3.hash(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has to be noted there, that we can use a cryptographically unsafe hash function for the Bloom-filter, we only care about an uniform distribution of the filter output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the class can be created finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter = bloom_filter(m, hash_function, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shakespeare-text was processed using the following Unix command:\n",
    "\n",
    "`tr -s  '[:blank:]\\n' '\\n' < shakespeare.txt | tr 'A-Z' 'a-z' | tr -cd [:lower:][:cntrl:] | sed '/^$/d' | sort | uniq > shakespeare_words.txt`\n",
    "\n",
    "in order to get a file containing only the separate, unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the Bloom filter is filled with all the words from *dict*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in open('dict'):\n",
    "    filter.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bloom(bloomfilter, filename):\n",
    "    not_in_filter = []\n",
    "    for word in open(filename):\n",
    "        if not filter.lookup(word.strip()):\n",
    "            not_in_filter.append(word.strip())\n",
    "    return not_in_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_in_filter = bloom(filter, 'shakespeare_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 800 words in the Shakespeare text, which are not in the Bloom filter.\n"
     ]
    }
   ],
   "source": [
    "print 'There are', len(not_in_filter), 'words in the Shakespeare text, which are not in the Bloom filter.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to jump to some conclusions, we chose some examples from this *not_in_filter* list above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, there are the words in the list, which are obviously spelt wrongly:\n",
    "- *abandond, accustomd, addressd, breakpromise, complaind, femalewhich* etc.\n",
    "\n",
    "But, also there are words present, which look right:\n",
    "- *abused, bled, egypt, feet*\n",
    "\n",
    "And names, which are not necessarily present in a dictionary of english words.\n",
    "- *adam, celia, charles* etc.\n",
    "\n",
    "Worth noting too, that the list also apparently contains many words in the plural form (ending with an *-s*) or the third person singular with the same *-s* ending. Obviously then, a proper spell checker implementation would work with the stems (roots??) of the words - but that is outside of the scope of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the approach with the Bloom filter will be compared to a simpler approach, which just compares the words of the dictionary to each of the distinct words of the Shakespeare text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_strip_dict(filename):\n",
    "    file_contents = []\n",
    "    for line in open(filename):\n",
    "        file_contents.append(line.strip())\n",
    "    return file_contents\n",
    "\n",
    "# storing all the words in a list\n",
    "english_dict = file_strip_dict('dict')\n",
    "\n",
    "# storing all uniques Shakespeare-words in another list\n",
    "shakespeare_list = file_strip_dict('shakespeare_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, the even simpler spellchecker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_spellcheck(input_words, dictionary):\n",
    "    not_found = []\n",
    "    for input_word in input_words:\n",
    "        if input_word not in dictionary:\n",
    "            not_found.append(input_word)\n",
    "    return not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_in_filter_2 = simple_spellcheck(shakespeare_list, english_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 928 words in the Shakespeare text, which are not in the dictionary.\n"
     ]
    }
   ],
   "source": [
    "print 'There are', len(not_in_filter_2), 'words in the Shakespeare text, which are not in the dictionary.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More words are found to be spelt wrongly using this method, which might look surprising, as we expected the Bloom filter to be \"worse\" due to using probabilities. However, it should be remembered again: the Bloom filter's answers mean something is **possibly in** the set, or **definitely not** in the set. Therefore, it could show that some wrongly spelt words are in the dictionary (meaning it passes them as correctly spelt), though they arent't there.\n",
    "Consequently, having less results here mean a worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further elaborate on the spell-checker, let's test briefly the \"usual\" approach for Bloom-filters too: checking the incoming word first against it, and in the case of a **possibly in** answer, looking it up in the dictionary, in order to verify whether the word really does exist in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def usual_bloom(bloomfilter, dictionary, words):\n",
    "    actually_not = []\n",
    "    not_in_bloom = []\n",
    "    for word in words:\n",
    "        if bloomfilter.lookup(word):\n",
    "            # checking against the dictionary\n",
    "            if word not in dictionary:\n",
    "                # it wasn't in the dictionary, so the Bloom filter misled us\n",
    "                actually_not.append(word)\n",
    "        else:\n",
    "            not_in_bloom.append(word)\n",
    "    \n",
    "    return actually_not, not_in_bloom              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bloom_miss, bloom_not = usual_bloom(filter, english_dict, shakespeare_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 words were not in the dictionary, even though the Bloom filter said they might be.\n",
      "800 words were not present in the Bloom filter.\n",
      "In total, 928 words of the Shakespeare text were not found in the dictionary.\n"
     ]
    }
   ],
   "source": [
    "print len(bloom_miss), 'words were not in the dictionary, even though the Bloom filter said they might be.'\n",
    "print len(bloom_not), 'words were not present in the Bloom filter.'\n",
    "print 'In total,', len(bloom_miss) + len(bloom_not), 'words of the Shakespeare text were not found in the dictionary.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method worked - and gave the same number of words as a result like with comparing the two files word-by-word.\n",
    "\n",
    "Thus, it is proven, that the Bloom filter gives *false positive* results - can be true for items, which are actually not a part of the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the speed difference?\n",
    "Let's take a look at the timings of the three different methods demonstrated above! The *%timeit* option of iPython was used, and while the exact time values are only representative to the computer this exercise was made on, their magnitude will be approximately the same on other systems too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *simple_spellcheck*: 1 loops, best of 3: 6.25 s per loop\n",
    "- *usual_bloom*: 1 loops, best of 3: 4.58 s per loop\n",
    "- *bloom* 100 loops, best of 3: 9.95 ms per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be concluded, that compared to the methods using lookups from a complete dictionary, the plain Bloom filter approach is almost a magnitude faster. It has the drawback of false positives on the other hand.\n",
    "The two other approaches are much slower, although it is worth noting that they are not optimized here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which words are reported to be in the dictionary but are actually not in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandonwhich',\n",
       " 'ages',\n",
       " 'alls',\n",
       " 'april',\n",
       " 'atomies',\n",
       " 'attending',\n",
       " 'audreyas',\n",
       " 'augmenting',\n",
       " 'bands',\n",
       " 'basest',\n",
       " 'bells',\n",
       " 'berhymed',\n",
       " 'boldend',\n",
       " 'brawls',\n",
       " 'breaks',\n",
       " 'brotherno',\n",
       " 'brows',\n",
       " 'burghers',\n",
       " 'burthen',\n",
       " 'cannons',\n",
       " 'coroners',\n",
       " 'cries',\n",
       " 'crookedpated',\n",
       " 'crowns',\n",
       " 'cupid',\n",
       " 'curvets',\n",
       " 'demonstrating',\n",
       " 'dennis',\n",
       " 'depends',\n",
       " 'diseases',\n",
       " 'dominions',\n",
       " 'earthquakes',\n",
       " 'england',\n",
       " 'entreaties',\n",
       " 'everas',\n",
       " 'evils',\n",
       " 'exits',\n",
       " 'extremest',\n",
       " 'fathers',\n",
       " 'features',\n",
       " 'fells',\n",
       " 'fields',\n",
       " 'filld',\n",
       " 'flattered',\n",
       " 'followed',\n",
       " 'forestborn',\n",
       " 'france',\n",
       " 'ganymede',\n",
       " 'gets',\n",
       " 'gipsies',\n",
       " 'goats',\n",
       " 'graces',\n",
       " 'grecian',\n",
       " 'hadst',\n",
       " 'hands',\n",
       " 'hanged',\n",
       " 'having',\n",
       " 'hawthorns',\n",
       " 'heads',\n",
       " 'heres',\n",
       " 'horses',\n",
       " 'humbled',\n",
       " 'husbands',\n",
       " 'inclined',\n",
       " 'joves',\n",
       " 'leavethe',\n",
       " 'lets',\n",
       " 'liest',\n",
       " 'liquors',\n",
       " 'looked',\n",
       " 'lookest',\n",
       " 'loved',\n",
       " 'lungs',\n",
       " 'maidens',\n",
       " 'mans',\n",
       " 'mingled',\n",
       " 'misprised',\n",
       " 'mossd',\n",
       " 'newscrammed',\n",
       " 'oerrun',\n",
       " 'offenders',\n",
       " 'overcame',\n",
       " 'overthrown',\n",
       " 'pains',\n",
       " 'peoples',\n",
       " 'persever',\n",
       " 'personae',\n",
       " 'pleased',\n",
       " 'puking',\n",
       " 'punished',\n",
       " 'rememberd',\n",
       " 'removedbear',\n",
       " 'revenues',\n",
       " 'runaways',\n",
       " 'rushes',\n",
       " 'saturdays',\n",
       " 'sawest',\n",
       " 'sayest',\n",
       " 'seest',\n",
       " 'semblances',\n",
       " 'sends',\n",
       " 'servants',\n",
       " 'sestos',\n",
       " 'shepherds',\n",
       " 'shins',\n",
       " 'sighd',\n",
       " 'soldiers',\n",
       " 'spurs',\n",
       " 'staind',\n",
       " 'stairs',\n",
       " 'strangers',\n",
       " 'stronger',\n",
       " 'suits',\n",
       " 'sundrys',\n",
       " 'sweetest',\n",
       " 'tastes',\n",
       " 'tellst',\n",
       " 'things',\n",
       " 'travellers',\n",
       " 'travels',\n",
       " 'troilus',\n",
       " 'tutord',\n",
       " 'videlicit',\n",
       " 'weights',\n",
       " 'whisperd',\n",
       " 'winters',\n",
       " 'wolves',\n",
       " 'writers']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom_miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 7.2 - Flajolet-Martin algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Flajolet-Martin* algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption which is logarithmic in the maximum number of possible distinct elements in the stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-pseudo-code, that is:\n",
    "\n",
    "```init BITMAP[] = 0\n",
    "for X in M:\n",
    "    index = r(hash(x)) // where R is the index of the first 1 seen from the LSB in the binary form\n",
    "    BITMAP[index] = 1```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the approximate number of unique values in *M* is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2^{R} / 0.77351$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where *R* is the last index (again, from LSB), where ```BITMAP[index] = 0```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm was made as a Python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bitstring import BitArray\n",
    "\n",
    "class flajolet_martin:\n",
    "    \n",
    "    def __init__(self, hashf, hashparam=None):\n",
    "        # size fixed to 32 bits\n",
    "        self.__bitarray_size = 32\n",
    "        \n",
    "        # initializing the bit array to zeroes\n",
    "        self.__bitarray = BitArray(self.__bitarray_size) # thus it is from 0 - 2**basize\n",
    "        \n",
    "        # taking the hash function. it should take two parameters:\n",
    "        # the string to be hashed, and the parameter of the hash, which will be\n",
    "        # just the no. of it now\n",
    "        self.__hash = hashf\n",
    "        \n",
    "        self.__hashp = hashparam\n",
    "    \n",
    "    def __trailing_zero(self, num):\n",
    "        if num == 0:\n",
    "            return 32 # Assumes 32 bit integer inputs!\n",
    "        p = 0\n",
    "        while (num >> p) & 1 == 0:\n",
    "            p += 1\n",
    "        return p\n",
    "    \n",
    "    def __first_zero(self, num):\n",
    "        ba = BitArray(uint=num, length=self.__bitarray_size)\n",
    "        ba.reverse()\n",
    "        \n",
    "        oi = 0\n",
    "        for bit in ba:\n",
    "            if bit == False:\n",
    "                break\n",
    "            oi += 1\n",
    "\n",
    "        return oi         \n",
    "    \n",
    "    def give_estimate(self):\n",
    "        fzi = self.__first_zero(self.__bitarray.int)\n",
    "        return (2 ** fzi) / 0.77351\n",
    "        \n",
    "    def process(self, element):\n",
    "        sf = self.__hash(element, self.__hashp) % 2 ** (self.__bitarray_size - 1)\n",
    "        self.__bitarray.set(1, self.__bitarray_size - 1 - self.__trailing_zero(sf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **MurmurHash3** function defined in *hash_function* above again when instantiating the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fm = flajolet_martin(hash_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shakespeare-text was processed again, this time without the removal of unique values:\n",
    "\n",
    "`tr -s  '[:blank:]\\n' '\\n' < shakespeare.txt | tr 'A-Z' 'a-z' | tr -cd [:lower:][:cntrl:] | sed '/^$/d' > shakespeare_all_words.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And read into a list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all of the words in *shakespeare_list_all* get processed by the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shakespeare_list_all = file_strip_dict('shakespeare_all_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in shakespeare_list_all:\n",
    "    fm.process(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2647.67100619\n"
     ]
    }
   ],
   "source": [
    "print fm.give_estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The result is *2647.67100619*, however, sort & uniq command gives back *3280* as the exact result - thus the value gotten using the *Flajolet-Martin* algorhtym is too large - it is a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.278322981934433"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((fm.give_estimate() - 3280) / 3280) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.28% error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the error, a common solution is to use the median which is less prone to be influenced by outliers. The problem with this is that the results can only take the form $2^{R} / 0.77351$, where *R* is integer. A common solution is to combine both the mean and the median: Create $k*l$ hash-functions and split them into *k* distinct groups (each of size *l*). Within each group use the median for aggregating together the *l* results, and finally take the mean of the *k* group estimates as the final estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a new hashing function is made to be passed to our *Flajolet-Martin* class; which uses $k*l$ hash-functions as described above, for each pass of hashing of the same data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3243.39698259\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "k = 10\n",
    "l = 10\n",
    "\n",
    "# first, generating k*l different random hash functions\n",
    "# which will be having a table of different random seeds for mmh3\n",
    "hashgroups = []\n",
    "for k_i in range(k):\n",
    "    hashseeds = []\n",
    "    for l_i in range(l):\n",
    "        hashseeds.append(random.randint(-2**31, 2**31))\n",
    "    hashgroups.append(hashseeds)\n",
    "\n",
    "ks = []\n",
    "for k in hashgroups:\n",
    "    ls = []\n",
    "    for l in k:\n",
    "        # Creating a FM class with this seed.\n",
    "        fm_kl = flajolet_martin(hash_function, l)\n",
    "        \n",
    "        for word in shakespeare_list_all:\n",
    "            fm_kl.process(word)\n",
    "\n",
    "        ls.append(fm_kl.give_estimate())\n",
    "        del fm_kl\n",
    "    ks.append(numpy.median(ls))\n",
    "\n",
    "print numpy.mean(ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is *3243.39698259* then, and the error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1159456528696825"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((numpy.mean(ks) - 3280) / 3280) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 1% - depending on the seed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "Finally, we can conclude, that the *Flajolet-Martin* algorhytm is a memory-efficient way to approximate the number of unique items in a stream. However, in its basic form (examined in the former part of this section), the values can be off - meaning errors well over 10% - due to some kind-of non-uniformness of the hash-functions, producing an off-value. Should be noted too, that since the approximation is $2^{R} / 0.77351$, the values are somewhat quantized to the powers of 2 too.\n",
    "\n",
    "In the latter part, the calculation was done using the \"median-trick\" again, by doing *k* groups having *l* different hash functions over the input, and taking the average of the median of the *l* functions. This greatly reduced the error, however of course it was much more CPU-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
